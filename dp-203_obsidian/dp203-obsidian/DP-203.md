---
tags:
  - DP-203
---
# [DP-203 Notes:](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#dp-203-notes)

### [2. Data Storage:](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#2-data-storage)
[[DataStorage]]
- [[Azure storage account]]
- [[Azure Blob Storage vs. Azure Data Lake Storage Gen2]]
[[Storage account access keys]]
- [[Storage account  shared access signature (SAS)]]
- [[Storage account Redundancy]]
   - [[ LRS]]
    - [[ZRS]]
    - [[GRS]]
    - [[Read-access GRS]]
    - [[GZRS]]
    - [[Read-access GZRS]]
- [[Storage account access tiers]]
    - [[Hot frequently access data]]
    - [[Cool infrequently access data]]
    - [[Archive]]: not availavble at storage account level, available at indivudual blob level
- [[Storage account]] -> Lifecycle management
### [3. T-SQL](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#3-t-sql)
[[T-SQL]]

- when using WHERE clause in a data warehouse -> use PARTITIONS in data warehouse -> to increase efficiency of SQL queries
### [4. Azure Synapse Analytics](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#4-azure-synapse-analytics)

##### [4.1 Azure Synapse](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#41-azure-synapse)

- features of azure synapse analytics
- compute options:
    - [[Serverless SQL pool]]
    - [[Dedicated SQL pool]]
        - [[DWU]] - datawarehousing unit
    - [[Apache spark pool]]

##### [4.2 External tables & Serverless SQL pool / Dedicated SQL pool](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#42-external-tables--serverless-sql-pool--dedicated-sql-pool)
[[External tables]]
[[Serverless SQL pool]]
[[Dedicated SQL pool]]
- steps to create and use external table
  
    - [[Create a database]] in synapse workspace
    - create a database master key with [[Encryption by password]]. this will be used to protect Shared Access Signature
    - use [[SAS]] to authorize the use of ADLS account, crate database scoped credential [[SasToken]]
    - [[Create external datasource]] (can be hadoop, blob storage, [[ADLS]])
    - create external file format object that defines the external data (file format = DELIMITEDTEXT or [[PARQUET]])
    - define the external table
    - use the [[Table for analysis]] (there will be a lag, as data is stored in external source)
    
    ```sql
    CREATE DATABASE SCOPED CREDENTIAL AzureStorageCredential
    WITH
    IDENTITY = 'ADLS-name',
    SECRET = 'ACCESS_KEY';
    
    -- In the SQL pool, we can use Hadoop drivers to mention the source
    
    CREATE EXTERNAL DATA SOURCE log_data
    WITH (    LOCATION   = 'abfss://data@ADLSNAME.dfs.core.windows.net',
            CREDENTIAL = AzureStorageCredential,
            TYPE = HADOOP
    )
    
    -- Drop the table if it already exists
    DROP EXTERNAL TABLE [logdata]
    
    -- Here we are mentioning the file format as Parquet
    
    CREATE EXTERNAL FILE FORMAT parquetfile  
    WITH (  
        FORMAT_TYPE = PARQUET,  
        DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'  
    );
    
    -- Notice that the column names don't contain spaces
    -- When Azure Data Factory was used to generate these files, the column names could not have spaces
    
    CREATE EXTERNAL TABLE [logdata]
    (
        [Id] [int] NULL,
        [Correlationid] [varchar](200) NULL,
        [Operationname] [varchar](200) NULL,
        [Status] [varchar](100) NULL,
        [Eventcategory] [varchar](100) NULL,
        [Level] [varchar](100) NULL,
        [Time] [datetime] NULL,
        [Subscription] [varchar](200) NULL,
        [Eventinitiatedby] [varchar](1000) NULL,
        [Resourcetype] [varchar](1000) NULL,
        [Resourcegroup] [varchar](1000) NULL
    )
    WITH (
    LOCATION = '/parquet/',
        DATA_SOURCE = log_data,  
        FILE_FORMAT = parquetfile
    )
    
    /*
    A common error can come when trying to select the data, here you can get various errors such as MalformedInput
    
    You need to ensure the column names map correctly and the data types are correct as per the parquet file definition
    
    */
    
    
    SELECT * FROM [logdata]
    ```
##### [4.3 Loading data into data warehouse (SQL pool)](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#43-loading-data-into-data-warehouse-sql-pool)
[[Warehouse]]
- using [[T-SQL COPY statement]]
    
- using azure [[Synapse pipeline]], can perform tranformations on data before copying the data to the warehouse
    
- using [[dp203-obsidian/AzureSynapse/Polybase]] to define external tables, use external tables to create the internal tables
    
- 1 - [[Load data using COPY statement]]
    
    - never use the admin account for load operations
    - create a seperate user for load operations
    - best practice - create a workload group - to segregate CPU percentage across groups of users
    - grant permissions
    - csv:
        
        ```sql
        COPY INTO logdata FROM 'https://appdatalake7000.blob.core.windows.net/data/Log.csv'
        WITH
        (
        FIRSTROW=2
        )
        ```
        
    - parquet:
        
        ```sql
        COPY INTO [logdata] FROM 'https://jibsyadls.blob.core.windows.net/data/raw/parquet/*.parquet'
        WITH
        (
        FILE_TYPE='PARQUET',
        CREDENTIAL=(IDENTITY= 'Shared Access Signature', SECRET='sv=2021-06-08&ss=b&srt=sco&sp=rl&se=2022-12-22T14:08:01Z&st=2022-12-22T06:08:01Z&spr=https&sig=WU%2FFh62PcCSx7wSEuccKC%2FdlgAwIto2aHJVXMiPovfM%3D')
        )
        ```
        
- 2 - [[Load data using external table using polybase]]
    
    ```sql
    CREATE TABLE [logdata]
    WITH
    (
    DISTRIBUTION = ROUND_ROBIN,
    CLUSTERED INDEX (id)   
    )
    AS
    SELECT  *
    FROM  [logdata_external];
    ```
    
- **3 - [[BULK INSERT from Azure Synapse]]
    
    - in azure data studio -> connect to external data
    - select ADLS gen2 (new linked service --- part of azure data factory)
    - creating connection to data store
    - fill the details and create
    - before copying data -> specific permissions have to be given to the storage account
    - go to Access Control in the data storage account
    - add a role assignment
    - role = storage blob data contributor (allows user to read and write data)
    - choose azure admin account & save
    - go to azure data studio -> linked -> select connected ADLS -> select the file
    - select the file, right click and select bulk load
    - this wil automatically create the SQL script

##### [4.4 Designing a data warehouse](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#44-designing-a-data-warehouse)
[[Designing a data warehouse]]
- fact table
    - contains measurable facts
    - usually large in size
- dimension table
- note: there is no concept of foreign key in sql data warehouse in dedicated SQL pool in azure synapse
- star schema
- ideal practice while building dimension tables:
    - dont have NULL values for properties in dimension table, wont give desired results when using reporting tools
    - try to replace NULL with some default value
- surrogate key: new key added in dimension table when mixing two different data sources with same primary keys
- can use "Identity column" feature in azure synapse to generate unique ID
- right approach -> take different tables & create fact table in synapse itself using ADF to migrate tables from azure database to azure synapse

##### [4.5 Transfer data from azure sql database to azure synapse](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#45-transfer-data-from-azure-sql-database-to-azure-synapse)
[[Transfer data from azure sql database to azure synapse]]
- [[Create table structure in synapse]]
- [[Open synapse studio  Integrate  Copy data tool]]
- [[Make connection with the source]] (azure sql database) - select the server, database and table details
- [[Make connection]] to the target (azure synapse) - select the required details
-[[Select the staging area in ADLS2 or blob]] - used by the copy statement

##### [4.6 Reading JSON files from ADLS/Blob](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#46-reading-json-files-from-adlsblob)
[[Reading JSON files from ADLS,BLob]]
```sql
-- Here we are using the OPENROWSET Function

SELECT TOP 100
    jsonContent
FROM
    OPENROWSET(
        BULK 'https://appdatalake7000.dfs.core.windows.net/data/log.json',
        FORMAT = 'CSV',
        FIELDQUOTE = '0x0b',
        FIELDTERMINATOR ='0x0b',
        ROWTERMINATOR = '0x0a'
    )
    WITH (
        jsonContent varchar(MAX)
    ) AS [rows]

-- The above statement only returns all as a single string line by line
-- Next we can cast to seperate columns

SELECT 
   CAST(JSON_VALUE(jsonContent,'$.Id') AS INT) AS Id,
   JSON_VALUE(jsonContent,'$.Correlationid') As Correlationid,
   JSON_VALUE(jsonContent,'$.Operationname') AS Operationname,
   JSON_VALUE(jsonContent,'$.Status') AS Status,
   JSON_VALUE(jsonContent,'$.Eventcategory') AS Eventcategory,
   JSON_VALUE(jsonContent,'$.Level') AS Level,
   CAST(JSON_VALUE(jsonContent,'$.Time') AS datetimeoffset) AS Time,
   JSON_VALUE(jsonContent,'$.Subscription') AS Subscription,
   JSON_VALUE(jsonContent,'$.Eventinitiatedby') AS Eventinitiatedby,
   JSON_VALUE(jsonContent,'$.Resourcetype') AS Resourcetype,
   JSON_VALUE(jsonContent,'$.Resourcegroup') AS Resourcegroup
FROM
    OPENROWSET(
        BULK 'https://appdatalake7000.dfs.core.windows.net/data/log.json',
        FORMAT = 'CSV',
        FIELDQUOTE = '0x0b',
        FIELDTERMINATOR ='0x0b',
        ROWTERMINATOR = '0x0a'
    )
    WITH (
        jsonContent varchar(MAX)
    ) AS [rows]
```
##### [4.6 Azure Synapse Architecture](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#46-azure-synapse-architecture)
[[Synapse Architecture]]
- there are 60 distributions
- data is shared into distributions to optimize the performance of work
- data and compute are seperate, they can scale independently
- **control node** - optimizes the query for parallel processing
- work is then passed to the **compute nodes**, these nodes will do the work in parallel

##### [4.7 Types of tables](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#47-types-of-tables)
[[Type of tables]]
- [[Round-robin distributed tables]]:
    
    - data is distributed randomly
    - default distribution while creating tables
    - best for temporary or staging tables
    - If there are no joins performed on tables, then you can consider using this table type
    - Also, if there is no clear candidate column for hash distributing the table.
- [[Hash-distributed tables]]:
    
    - data is distributed based on HASH()
    - good for large tables - fact tables
    - while choosing distribution column: - ensure it has many unique values - data gets spread across more distributions - if not, it may result in DATA SKEW - dont use date column - does not have NULLS or very few NULLS - is used in JOIN, GROUP BY and HAVING clauses - is not used in the WHERE clause
    - ```sql
             CREATE TABLE [dbo].[SalesFact](
             [ProductID] [int] NOT NULL,
             [SalesOrderID] [int] NOT NULL,
             [CustomerID] [int] NOT NULL,
             [OrderQty] [smallint] NOT NULL,
             [UnitPrice] [money] NOT NULL,
             [OrderDate] [datetime] NULL,
             [TaxAmt] [money] NULL
             )
             WITH  
             (   
                 DISTRIBUTION = HASH (CustomerID)
             )
        ```
        
 Replicated tables:
    
    - full copy of table is cached on every distribution (compute node)
    - good for dimension tables
    - ideal for tables less than 2 GB
    - not ideal for tables with frequent insert, update and delete
    - Use replicated tables for queries with simple query predicates, such as equality or inequality
    - Use distributed tables for queries with complex query predicates, such as LIKE or NOT LIKE
    - ```sql
          CREATE TABLE [dbo].[SalesFact](
          [ProductID] [int] NOT NULL,
          [SalesOrderID] [int] NOT NULL,
          [CustomerID] [int] NOT NULL,
          [OrderQty] [smallint] NOT NULL,
          [UnitPrice] [money] NOT NULL,
          [OrderDate] [datetime] NULL,
          [TaxAmt] [money] NULL
          )
          WITH  
          (   
              DISTRIBUTION = REPLICATE
          )
        ```
        
- If we are not using hash-distributed tables for fact tables & replicated tables for dimension tables, while performing JOINs or any other operations - data has to be moved from one distribution to the other distribution. this operation is called as "**DATA SHUFFLE MOVE OPERATION**" - this may lead to time lag for very big tables.
    
##### [4.8 Surrogate keys for dimension tables](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#48-surrogate-keys-for-dimension-tables)
[[Surrogate keys for dimension tables]]
- surrogate key == non-business key
- simple incrementing integer values
- in SQL pool tables, use IDENTITY column feature

```sql
CREATE TABLE [dbo].[DimProduct](
	[ProductSK] [int] IDENTITY(1,1) NOT NULL,
	[ProductID] [int] NOT NULL,
	[ProductModelID] [int] NOT NULL,
	[ProductSubcategoryID] [int] NOT NULL,
	[ProductName] varchar(50) NOT NULL,
	[SafetyStockLevel] [smallint] NOT NULL,
	[ProductModelName] varchar(50) NULL,
	[ProductSubCategoryName] varchar(50) NULL
)
```

- in synapse studio integrate data copy method -> the Identity column - not oncremented one by one - but by number of distributions
- ADF can properly create incremental nubers in IDENTIY column

##### [4.9 Slowly changing dimensions](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#49-slowly-changing-dimensions)
[[Slowly changing dimensions]]
- type-1 SCD: updates the OLD value with the NEW value in the data warehouse
- type-2 SCD: keeps both OLD and NEW values (start_date and end_date and is_active)
- type-3 SCD: instead of having multiple rows, additional columns are added to signify the change

##### [4.10 Heap tables](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#410-heap-tables)
 [[Heap tables]]
```sql
CREATE TABLE [dbo].[SalesFact_staging](
	[ProductID] [int] NOT NULL,
	[SalesOrderID] [int] NOT NULL,
	[CustomerID] [int] NOT NULL,
	[OrderQty] [smallint] NOT NULL,
	[UnitPrice] [money] NOT NULL,
	[OrderDate] [datetime] NULL,
	[TaxAmt] [money] NULL
)
WITH(HEAP,
DISTRIBUTION = ROUND_ROBIN
)

CREATE INDEX ProductIDIndex ON [dbo].[SalesFact_staging] (ProductID)
```

- this does not create a clustered column store table
- clustered column store table: used for final tables
- for temporary tables - HEAP tables are prefered
- In heap tables - no option to create clustered column store INDEX
- so, we can create a non-clustered INDEX using `CREATE INDEX`
##### [4.11 Partitions](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#411-partitions)
[[dp203-obsidian/AzureSynapse/Partitions]]
```sql
-- Let's create a new table with partitions
CREATE TABLE [logdata]
(
    [Id] [int] NULL,
	[Correlationid] [varchar](200) NULL,
	[Operationname] [varchar](200) NULL,
	[Status] [varchar](100) NULL,
	[Eventcategory] [varchar](100) NULL,
	[Level] [varchar](100) NULL,
	[Time] [datetime] NULL,
	[Subscription] [varchar](200) NULL,
	[Eventinitiatedby] [varchar](1000) NULL,
	[Resourcetype] [varchar](1000) NULL,
	[Resourcegroup] [varchar](1000) NULL
)
WITH
(
PARTITION ( [Time] RANGE RIGHT FOR VALUES
            ('2021-04-01','2021-05-01','2021-06-01')

   )  
)
```

**Switching partitions**

```sql
ALTER TABLE [logdata] SWITCH PARTITION 2 TO [logdata_new] PARTITION 1;
```
##### [4.12 Indexes](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#412-indexes)

[[Indexes]]
- Clusterd Columnstore Indexes
- Heap tables
- Clustered Indexes
- NonClustered Indexes
### [5. Design and Develop Data Processing - Azure Data Factory](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#5-design-and-develop-data-processing---azure-data-factory)
[[DataFactory]]
##### [5.1 Azure Data Factory](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#51-azure-data-factory)

- cloud-based ET tool
- data-driven orchestrated workflows

[[ADF components]]

- **[[Linked Service]]:** can create required compute resources to enable ingestion of data from the source
    
- **[[Datasets]]:** represents the data structure within the data store that is being referenced by the Linked Service object
    
- **[[Activity]]:** contains the actual transformation logic
    
- azure pipeline will create compute infrastructure known as **[[Integration runtime]]** - responsible for taking data from source and copying it to the destination

##### [5.2 Mapping Data Flows](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#52-mapping-data-flows)
[[Mapping Data Flow]]
- This helps to visualize the data transformations in Azure Data Factory.
- Here you can write the required transformation logic without actually writing any code.
- The data flows are run on **[[Apache Spark clusters]]**.
- Here Azure Data Factory will handle the transformations in the data flow.
- **[[Debug mode]]** – You also have a Debug mode in place. Here you can actually see the results of each transformation.
- In the debug mode session, the data flow is run interactively on a Spark cluster.
- minimum cluster size to run a Data Flow is 8 vCores.

##### [5.3 Self-Hosted Integration runtime](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#53-self-hosted-integration-runtime)

- if the database in own custom system sitting inside a VM
- install the integration runtime on VM
- register the server with the data factory

### [6. Azure Event Hubs and Streaming Analytics](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#6-azure-event-hubs-and-streaming-analytics)
[[Event Hubs and Streaming Analytics]]
##### [6.1 Azure Event Hubs](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#61-azure-event-hubs)

- [[Big data streaming platform]]
- can receive and process millions of events per second
- can stream log data, telemetry data, any sort of events to azure events hub
- [[Event hubs namespace]]-> event hubs
- event hubs - multiple partitions - ingest more data at a time - event receivers can take data from one partition or multiple partitions - helps event receivers to consume data in faster rate

**[[Components of Azure event hubs]]:**

- [[Event producers]]: entity that sends data to event hub - events can be published using the protocols - HTTPS, AMQP, Apache Kafka
- [[dp203-obsidian/AzureSynapse/Partitions]]: data is split across partitions - allows for better throughput of data onto event hubs
- [[Consumer groups]]: view (state, position or offset) of an entire event hub
- [[Throughput]]: controls the throughput capacity of event hubs
- [[Receivers]]: entity taht reads event data

### [7. Spark Pool](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#7-spark-pool)
[[Spark Pool]]
##### [7.1 Azure Synapse - Apache Spark pool](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#71-azure-synapse---apache-spark-pool)

- [[Serverless spark pool]]
- [[Not charged on creation of pool]]
- [[Charged when underlying jobs are running]]
- [[Large datasets and distribute computation across multiple pools]]
- [[Driver node and executors]]
- [[Spark scala]]
- [[Creates RDD - Resilient Distributed Dataset]]

```scala
val dist = sc.parallelize(data)
```

##### [7.2 Spark Dataset](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#72-spark-dataset)
[[Spark Dataset]]
- This is a strongly typed collection of domain-specific objects
- This data can then be transformed in parallel
- Normally you will perform either transformations or actions on a dataset
- The transformation will produce a new dataset
- The action will trigger a computation and produce the required result
- The benefit of having a Dataset is that you can use powerful transformations on the underlying data

##### [7.3 Spark Dataframe](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#73-spark-dataframe)
[[Spark Dataframe]]
- The DataFrame is nothing but a Dataset that is organized into named columns.
- Its like a table in a relational database.
- You can construct DataFrames from external files.
- When it comes to Datasets, the API for working with Datasets is only available for Scala and Java.
- For DataFrames, the API is available in Scala, Java, Python and R.
- In the spark pool, the spark instances are created when you connect to a spark pool, create a session and run a job
- when you submit another job, if there is capacity in the pool and the spark instance has spare capacity, itwill run the 2nd job
- else, it will crate a new spark instance to run the job
    
##### [7.4 Spark table](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#74-spark-table)
[[Spark table]]
- stored in metastore of spark pool ([[HIVE META STORE]])
- not for storing data, just for temporary tables
- the benefit of spark table: metastore is shared with serverless SQL pool as well

```scala
%%spark
val df = spark.read.sqlanalytics("jibsypool.dbo.logdata") 
df.write.mode("overwrite").saveAsTable("logdatainternal")

%%sql
SELECT * FROM logdatainternal
```

##### [7.5 Spark tables - Creation](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#75-spark-tables---creation)
[[Spark tables creation]]
- spark tables are parquet based tables

```scala
%%sql
CREATE DATABASE internaldb
CREATE TABLE internaldb.customer(Id int,name varchar(200)) USING Parquet

%%sql
INSERT INTO internaldb.customer VALUES(1,'UserA')

%%sql
SELECT * FROM internaldb.customer


// If you want to load data from the log.csv file and then save to a table
%%pyspark
df = spark.read.load('abfss://data@datalake2000.dfs.core.windows.net/raw/Log.csv', format='csv', header=True)
df.write.mode("overwrite").saveAsTable("internaldb.logdatanew")

%%sql
SELECT * FROM internaldb.logdatanew
```

- to delete the database, tables have to be dropped first

##### [7.6 Spark Pool - JSON files](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#76-spark-pool---json-files)
[[Spark Pool]]
```scala
%%spark

val df = spark.read.format("json").load("abfss://data@datalake2000.dfs.core.windows.net/raw/customer/customer_arr.json")
display(df)

// Now we need to expand the courses information

%%spark
import org.apache.spark.sql.functions._
val df = spark.read.format("json").load("abfss://data@datalake2000.dfs.core.windows.net/raw/customer/customer_arr.json")
val newdf=df.select(col("customerid"),col("customername"),col("registered"),explode(col("courses")))
display(newdf)

// Reading the customer object file
%%spark
import org.apache.spark.sql.functions._
val df = spark.read.format("json").load("abfss://data@datalake2000.dfs.core.windows.net/raw/customer/customer_obj.json")
val newdf=df.select(col("customerid"),col("customername"),col("registered"),explode(col("courses")),col("details.city"),col("details.mobile"))
display(newdf)
```

### [8. Databricks](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#8-databricks)
[[Databricks]]
##### [8.1 Databricks](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#81-databricks)

- makes use of apache spark to provide a unified analytics platform
- creates the underlying compute infra
- has its own underlying file system - abstraction of an underlying storage layer
- will install spark by itself - also has comatibility for other libs - ML libs
- provides workspace - notebooks with collaboration and visualization features

##### [8.2 Azure Databricks](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#82-azure-databricks)

- completely azure-managed environment
- makes use of underlying compute infrastructure and virtual networks
- makes use of azure security - azure active directory and role-based access control

**Clusters in Azure Databricks**

- inside cluster - 2 types of nodes
    
    - [[Worker nodes]] - perform the underlying tasks
    - [[Driver node]] - distributes the task to worker nodes
    - [[Types of clusters]]
    
    - [[Interactive clusters]]: interactive notebooks and multiple users can use a cluster for collaboration
    - Job cluster: cluster is started when the job has to run, and will be terminated once the job is completed
- 2 types of Interactive cluster
    
    - Standard cluster:
        - recommended if you are a single user
        - no fault isolation - if multiple users are using and one user has fault - this might impact workloads of other users
        - resources of a cluster might get allocated to a single workload
        - has support for python, R, SQL and Scala
    - High concurrency cluster:
        - for multiple users
        - fault isolation
        - resources are shared across different user workloads
        - support for python, R and SQL (no scala)
        - table access control: can grant and revke access to data from python and SQL

##### [8.3 Autoscaling a cluster](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#83-autoscaling-a-cluster)

- When creating an Azure Databricks cluster, you can specify a minimum and maximum number of workers for the cluster.
- Databricks will then choose the ideal number of workers to run the job.
- If a certain phase of your job requires more compute power, the workers will be assigned accordingly.
- There are two types of autoscaling
    - **[[Standard autoscaling]]**
        - Here the cluster starts with 8 nodes
        - Scales down only when the cluster is completely idle and it has been underutilized for the last 10 minute
        - Scales down exponentially, starting with 1 node
    - **[[Optimized autoscaling]]**
        - This is only available for Azure Databricks Premium Plan
        - Can scale down even if the cluster is not idle by looking at shuffle file state
        - Scales down based on a percentage of current nodes
        - On job clusters, scales down if the cluster is underutilized over the last 40 seconds
        - On all-purpose clusters, scales down if the cluster is underutilized over the last 150 seconds

##### [8.4 Azure Databricks Table](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#84-azure-databricks-table)
[[Databricks Table]]
- In Azure Databricks, you can also create a database and tables
- The table is a collection of structured data
- You can then perform operations on the data that are supported by Apache Spark on DataFrames on Azure Databricks tables
- There are two types of tables – global and local tables.
- A global table is available across all clusters
- A global table is registered in the Azure Databricks Hive metastore or an external metastore
- The local table is not accessible from other clusters and is not registered in the Hive metastore

##### [8.5 Delta Lake](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#85-delta-lake)
[[Delta Lake]]
- ACID transactions on Spark - Serializable isolation levels ensure that readers never see inconsistent data
- Scalable metadata handling - Leverages Spark distributed processing power to handle all the metadata for petabyte-scale tables with billions of files at ease.
- Streaming and batch unification - A table in Delta Lake is a batch table as well as a streaming source and sink. Streaming data ingest, batch historic backfill, interactive queries all just work out of the box.
- Schema enforcement - Automatically handles schema variations to prevent insertion of bad records during ingestion.
- Time travel - Data versioning enables rollbacks, full historical audit trails, and reproducible machine learning experiments.
- Upserts and deletes - Supports merge, update and delete operations to enable complex use cases like change-data-capture, slowly-changing-dimension (SCD) operations, streaming upserts, and so on.

### [9 Security](https://github.com/jithendray/dp203-azure-data-engineering/blob/main/README.md#9-security)
[[Security]]
- **[[Azure Key Vault]]** - certificates, encryption keys and secrets (passwords and login details)
    
- **[[Azure Data Factory – Encryption]]**
    
    - Azure Data Factory already encrypts data at rest which also includes entity definitions and any data that is cached.
    - The encryption is carried out with Microsoft-managed keys.
    - But you can also define your own keys using the Azure Key vault service.
    - For the key vault, you have to ensure that **Soft delete** is enabled and the setting of **Do Not Purge** is also enabled.
    - Also grant Azure Data Factory the key permissions of 'Get', 'Unwrap Key' and 'Wrap Key'
- **[[Azure Synapse - Data Masking]]**
    
    - Here the data in the table can be limited in its exposure to non-privileged users.
    - You can create a rule that can mask the data.
    - Based on the rule you can decide on the amount of data to expose to the user.
    - There are different masking rules.
    - Credit Card masking rule – This is used to mask the column that contain credit card details. Here only the last four digits of the field are exposed.
    - Email – Here first letter of the email address is exposed. And the domain name of the email address is replaced with XXX.com.
    - Custom text- Here you decide which characters to expose for a field.
    - Random number- Here you can generate a random number for the field.
- **[[Azure Synapse - Auditing]]**
    
    - You can enable auditing for an Azure SQL Pool in Azure Synapse Analytics.
    - This feature can be used to track database events and write them to an audit log.
    - The logs can be stored in an Azure storage account, a Log Analytics workspace and Azure Event Hubs.
    - This helps in regulatory compliance. It helps to gain insights on any anomalies when it comes to database activities.
    - Auditing can be enabled at the data warehouse level or server level.
    - If it is applied at the server level, then it will be applied to all of the data warehouses that reside on the server
- **[[Azure Synapse - Data Discovery and Classification]]**
    
    - This feature provides capabilities for discovering, classifying, labelling, and reporting the sensitive data in your databases.
    - The data discovery feature can scan the database and identify columns that contains sensitive data. You can then view and apply the recommendations accordingly.
    - You can then apply sensitivity labels to the column. This helps to define the sensitivity level of the data stored in the column.
- **[[Row level security]]**
    

```sql
-- Create a new schema for the security function

CREATE SCHEMA Security;  

-- Create an inline table function
-- The function returns 1 when a row in the Agentcolumn is the same as the user executing the query 
-- (@Agent = USER_NAME()) or if the user executing the query is the Manager user (USER_NAME() = 'Supervisor').

CREATE FUNCTION Security.securitypredicate(@Agent AS nvarchar(50))  
    RETURNS TABLE  
WITH SCHEMABINDING  
AS  
    RETURN SELECT 1 AS securitypredicate_result
WHERE @Agent = USER_NAME() OR USER_NAME() = 'Supervisor';  

-- Create a security policy adding the function as a filter predicate. The state must be set to ON to enable the policy.

CREATE SECURITY POLICY Filter  
ADD FILTER PREDICATE Security.securitypredicate(Agent)
ON [dbo].[Orders] 
WITH (STATE = ON);  
GO

-- Lab - Azure Synapse - Row-Level Security

-- Allow SELECT permissions to the function

GRANT SELECT ON Security.securitypredicate TO Supervisor;
GRANT SELECT ON Security.securitypredicate TO AgentA;  
GRANT SELECT ON Security.securitypredicate TO AgentB;
```

- **[[Azure Synapse - Column level security]]**

```sql
CREATE USER Supervisor WITHOUT LOGIN;  
CREATE USER UserA WITHOUT LOGIN;  

-- Grant access to the tables for the users

GRANT SELECT ON [dbo].[Orders] TO Supervisor; 
GRANT SELECT ON [dbo].[Orders](OrderID,Course,Quantity) TO UserA; 
```

### [DP-203 - Data Engineering on Microsoft Azure 2023](https://www.udemy.com/course/data-engineering-on-microsoft-azure/)

Prepare for your DP-203 exam

    
- Introduction to Cloud Computing    
- Azure and the Azure Free Account
- Creating an Azure Free Account
- Quick tour of the Azure Portal
- Quick note - Security Defaults

### UPDATED - Design and implement data storage - Basics15 aulas • 1h 5m

- Understanding data
- Example of data storage
- Azure Storage accounts
- Azure SQL databases
- Application connecting to Azure Storage and SQL database  
- Azure Data Lake Gen-2 storage accounts
- Creating an Azure Data Lake Gen-2 storage account
- Different file formats
- Lab - Uploading data to Azure Data Lake Gen2
    
### UPDATED - Design and implement data storage - Overview on Transact-SQL14 aulas • 37m


 - The internals of a database engine
-  Setting up a new Azure SQL database
-  Setting up SQL Server Management Studio
-  T-SQL - SELECT clause
-  T-SQL - WHERE clause
-  T-SQL - ORDER BY clause
-  T-SQL - Aggregate Functions  
-  T-SQL - GROUP BY clause
-  T-SQL - HAVING clause
-  Quick Review on Primary and Foreign Keys
-  T-SQL - Creating Tables with Keys
-  Lab - T-SQL - Table Joins
 
### UPDATED - Design and implement data storage - Azure Synapse Analytics57 aulas • 4h 40m

-  Code for this section
-  Section Introduction
 - The pre-requisite resources we require
-  What have we seen so far
-  A data warehouse
-  Welcome to Azure Synapse Analytics
-  Let's create a Azure Synapse workspace
-  Azure Synapse - Compute options
-  Let's open up some data
-  Using External tables - CSV - Part 1
-  Using External tables - CSV - Part 2
-  External Tables - Parquet file
-  External Tables - Multiple Parquet files
-  OPENROWSET
-  Creating a SQL pool
-  SQL Pool - External Tables - Parquet
-  SQL Pool - External Tables - CSV
-  Pausing the SQL Pool
-  Loading data into the Dedicated SQL Pool
-  Loading data into a table - COPY Command - CSV
-  Loading data into a table - COPY Command - Parquet
-  Loading data - Bulk Load - Quick Note
-  [[Loading data using Poly Base]]
-  Loading data - Pipelines - Storage accounts
-  Loading data - Pipelines - Azure SQL database
-  Designing a data warehouse
-  More on dimension tables
-  Building a Fact Table
-  Building a dimension table
 - Transfer data to our SQL Pool
-  Using Power BI for Star Schema
-  Understanding Azure Synapse Architecture
-  Understanding table types
-  About the staging area
-  Compute to data distribution
- Note on the SQL pools
-  Understanding Round-Robin tables
-  Creating Hash-distributed Tables
-  Note on creating replicated tables
-  Designing your tables
-  Hash distribution for multiple columns
-  Lab - Example when using the right distributions for your tables
-  Good Practices when designing tables
-  Lab - Surrogate keys for dimension tables
-  Slowly Changing dimensions
-  Indexes
-  Creating Indexes
-  Which Method to choose  
-  Creating a heap table 
-  Partitions in Azure Synapse
-  Creating a table with partitions  
-  Switching partitions
    
    04:08
    
- Lab - Reading JSON files
    
    04:53
    
- Lab - Windowing Functions
    
    03:28
    
- Lab - CASE statement
    
    02:11
    
- Quick Note on what we are taking forward to the next sections
    
    00:54
    
- What about the Spark Pool
    
    00:43
    

### UPDATED - Design and Develop Data Processing - Azure Data Factory50 aulas • 4h 38m

- Section Introduction
    
    00:52
    
- Code for this section
    
    00:04
    
- Extract, Transform and Load
    
    03:02
    
- What is Azure Data Factory
    
    07:40
    
- Starting with Azure Data Factory
    
    02:56
    
- Lab - Azure Data Lake to Azure Synapse - Log.csv file
    
    10:32
    
- Lab - Generating a Parquet file
    
    08:55
    
- Review on what has been done so far
    
    03:36
    
- Lab - Two step process for the pipeline
    
    08:22
    
- Note on data types in source
    
    01:42
    
- So if you want to change the source data type
    
    03:06
    
- Lab - What about using a query for data transfer
    
    05:02
    
- More information on the copy data activity
    
    00:53
    
- Lab - Adding an extra column
    
    04:09
    
- Lab - Copy Data - Using the Copy command
    
    03:38
    
- Lab - Copy Data - Using Polybase
    
    03:15
    
- Mapping Data Flow
    
    03:12
    
- Lab - Mapping Data Flow - Fact Table
    
    09:26
    
- Lab - Pipeline - Mapping data flows
    
    04:53
    
- Lab - Mapping Data Flow - Dimension Table - DimCustomer
    
    02:40
    
- Lab - Mapping Data Flow - Dimension Table - DimProduct
    
    08:57
    
- Lab - Derived Column
    
    10:24
    
- Lab - Surrogate Keys - Dimension tables
    
    06:49
    
- Lab - Cache Sink - The first step
    
    06:24
    
- Lab - Cache Sink - Keeping the data ready
    
    05:00
    
- Lab - Cache Sink Implementation
    
    13:39
    
- Data Flow Debug feature
    
    05:00
    
- Lab - Handling Duplicate rows
    
    08:09
    
- Lab - Filtering rows
    
    07:28
    
- Lab - Generating JSON data
    
    04:47
    
- Lab - Loading JSON into SQL Pool
    
    05:03
    
- Lab - Processing JSON Arrays
    
    09:11
    
- Lab - Processing JSON objects
    
    05:20
    
- Self-Hosted Integration Runtime
    
    01:48
    
- Lab - Self-Hosted runtime - Building the machine
    
    05:43
    
- Lab - Self-Hosted Runtime - Setting up nginx
    
    03:53
    
- Lab - Self-Hosted Runtime - Setting up the runtime
    
    04:13
    
- Lab - Self-Hosted Runtime - Copy Activity
    
    05:32
    
- Lab - Self-Hosted Runtime - Mapping Data Flow
    
    11:39
    
- Lab - Conditional Split
    
    06:13
    
- About Schema Drift
    
    06:07
    
- Azure Data Factory and Git
    
    09:27
    
- Quick Note on other important aspects
    
    04:28
    
- Note on partitions in the copy process
    
    02:25
    
- Lab - Quick look at the web activity
    
    04:58
    
- Lab - Get Metadata Activity
    
    03:33
    
- Lab - For Each Activity
    
    11:43
    
- Lab - Stored Procedures
    
    05:32
    
- Lab - Using the Lookup Activity
    
    05:35
    
- What we still need to consider
    
    01:15
    

### UPDATED -Design and Develop Data Processing - Azure Event Hubs ,Stream Analytics41 aulas • 3h 39m

- Code for this section
    
    00:04
    
- What are we going to cover
    
    01:09
    
- Batch and Real-Time Processing
    
    04:24
    
- What are Azure Event Hubs
    
    03:33
    
- Lab - Creating an instance of Event hub
    
    02:40
    
- Lab - .NET - Sending events
    
    04:35
    
- Lab - .NET - Receiving events
    
    03:15
    
- So the purpose of Azure Event Hubs
    
    08:56
    
- What is Azure Stream Analytics
    
    02:01
    
- Lab - Creating a Stream Analytics job
    
    01:42
    
- Lab - Azure Stream Analytics - Defining the job
    
    13:04
    
- Review on what we have seen so far
    
    03:27
    
- Lab - Setting diagnostics logs for Azure Data Lake
    
    12:21
    
- Lab - Sending data to the Event Hub
    
    08:26
    
- Lab - Formulating our query
    
    08:25
    
- Lab - Reading the Blob Diagnostic data
    
    05:18
    
- Lab - Azure Event Hubs Capture
    
    04:34
    
- Lab - Azure Web App - Diagnostic setting
    
    07:21
    
- Lab - Formulating the query
    
    04:49
    
- Lab - Running our Stream Analytics job
    
    09:31
    
- Debugging your jobs
    
    02:16
    
- About Windowing functions
    
    04:11
    
- Lab - Using the Tumbling window
    
    09:15
    
- Using System.Timestamp
    
    01:49
    
- Having multiple queries in the job
    
    01:43
    
- Quick note on other windowing functions
    
    02:10
    
- Quick Note on other important aspects
    
    05:25
    
- Lab - Reference data
    
    06:59
    
- Lab - Power BI Output
    
    09:37
    
- Signing up for Power BI
    
    00:51
    
- Lab - Reading Network Security Group Logs - Server Setup
    
    04:14
    
- Lab - Reading Network Security Group Logs - Enabling NSG Flow Logs
    
    01:47
    
- Understanding the NSG Flow Log structure
    
    04:57
    
- Starting with the query
    
    03:23
    
- Formulating our query
    
    09:32
    
- Finalizing our query
    
    07:27
    
- Revisiting Azure Data Factory - Mapping Data Flow
    
    11:06
    
- Azure Data Factory - Copying to sink
    
    05:38
    
- Azure Data Factory - Deleting files
    
    02:48
    
- Azure Data Factory - Storage Event trigger
    
    04:33
    
- Azure Data Factory - Tumbling Windows trigger
    
    09:48
    

### UPDATED - Design and Develop Data Processing - Scala, Notebooks and Spark31 aulas • 1h 48m

- Section Introduction
    
    02:08
    
- Code for this section
    
    00:04
    
- Introduction to Scala
    
    01:03
    
- Installing Scala
    
    03:29
    
- Getting started with Scala
    
    04:11
    
- Using Variables
    
    03:30
    
- Scala - for construct
    
    01:54
    
- Scala - while construct
    
    01:37
    
- Scala - case construct
    
    03:48
    
- Scala - Functions
    
    01:17
    
- Scala - List collection
    
    04:09
    
- Starting with Python
    
    01:49
    
- Python - A simple program
    
    01:37
    
- Python - List collection
    
    02:11
    
- Python - If construct
    
    01:04
    
- Python - while construct
    
    00:54
    
- Python - Functions
    
    02:20
    
- Quick look at Jupyter Notebook
    
    04:07
    
- Lab - Azure Synapse - Creating a Spark pool
    
    07:10
    
- Lab - Spark Pool - Starting out with Notebooks
    
    04:59
    
- Lab - Spark Pool - With Python
    
    06:57
    
- Lab - Spark Pool - Load data
    
    08:24
    
- Lab - Spark Pool - Grouping your results
    
    01:31
    
- Lab - Creating a temporary view
    
    03:57
    
- Lab - Spark Pool - Write data to Azure Synapse
    
    13:43
    
- Spark Pool - Write data to Azure Synapse - Python code
    
    00:57
    
- Spark Pool - Combined Power
    
    02:01
    
- Lab - Databases and tables
    
    03:21
    
- Lake databases
    
    01:54
    
- Azure Synapse[[ Database templates]]
    
    02:04
    
- Lab - Azure Synapse Link for Azure Cosmos DB
    
    09:40
    

### UPDATED - [[Design and Develop Data Processing - Azure Databricks]] 29 aulas • 1h 50m

-  [[What are we going to cover]]
-  [[What is Azure Data bricks]]
-  [[Concepts with Azure Data bricks]]
-  [[Creating a workspace]]
-  [[Creating a cluster]]
-  [[Loading data from a file]]
-  [[Group By and Visualizations]]
-  [[Few functions on dates]]
-  [[Filtering on NULL values ]]
-  [[Saving to a table ]]
-  [[Reading from Azure Data Lake]]
-  [[JSON-based files]]
-  [[Processing JSON-based files - Another example ]] 
-  [[Using the COPY INTO command  ]]
-  [[Streaming data using Azure Data bricks]]
-  [[Removing duplicate rows]]
-  [[Specifying the schema]]
-  [[More on clusters]]
-  [[Versioning of tables]]
-  [[Reading data from Azure Synapse ]] 
-  [[Writing data to Azure Synapse SQL Dedicated Pool]]
-  [[Different modes when writing data ]]
-  [[Merging data]]
-  [[Running an automated job]]
-  [[Azure Data Factory - Running a notebook]]
-  [[Streaming from Azure Event Hub - Setup ]] 
-  [[Streaming from Azure Event Hub - Implementation ]]
-  [[Deleting the workspace]]

### UPDATED - [[Design and Implement Data Security]] 24 aulas • 1h 40m

- [[DataSecurity]]
- [[Azure Data Lake Gen 2 Security - Account Keys]] 
- [[Using the Azure Storage Explorer]]   
- [[Azure Data Lake Gen 2 Security - Shared Access Signature]]
- [[Using Azure Active Directory]]
- [[Granting access via Azure AD ]] 
- [[Using Access Control Lists]]
- [[Look back at how we were accessing the data lake account ]]
- [[Azure Data bricks - Secret Scope - Key Vault   ]]
-[[ Azure Data bricks - Secret Scope - Implementation]]
-[[ Other ways of connecting to Azure storage ]]
- [[About Managed Identities]]
- [[Azure Data Factory - Managed Identity]]
- [[Azure Storage Accounts - Network and Firewall]]
- [[Azure Storage Accounts - Virtual Network Service Endpoint]]
- [[Azure Data Factory - Encryption]]
- [[Azure Synapse Encryption  ]]
- [[Azure Synapse - Data Masking  ]]
- [[Azure Synapse - Column-Level Security]]
- [[Azure Synapse - Row-Level Security ]] 
- [[Azure Synapse - Azure AD Authentication]]
- [[Azure Synapse - Azure AD Authentication - Setting the admin]]
- [[Azure Synapse - Azure AD Authentication - Creating a user]]
- [[Azure Synapse - Data Discovery and Classification]]

# Azure Synapse Analytics
- [[Create and setup a Synapse workspace]]
(https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-create-workspace)
- [[Analyze using a serverless SQL pool]]
(https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-sql-on-demand)
- [[Analyze using a Data Explorer pool]]
(https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-data-explorer)
- [[Analyze using Apache Spark]]
(https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-spark)
- [[Analyze using a dedicated SQL pool]]
(https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-sql-pool)
- [[Analyze data in a storage account]]
(https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-storage)
-[[Orchestrate with pipelines]]
(https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-pipelines)
- [[Visualize data with Power BI]]
(https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-visualize-power-bi)
- [[Monitor activities]]
(https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-monitor)
-[[Explore the Knowledge center]]
(https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-knowledge-center)
- [[Add an administrator]]
(https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-add-admin)

[[Data Explorer]]

### UPDATED - Monitor and [[Optimize data storage and data processing]] 27 aulas • 1h 19m

- [[Microsoft Purview]]
- [[Microsoft Purview - Azure Data Lake]]
- [[Microsoft Purview - Azure Data Factory]]
- [[Best practices for structing files in your data lake]]
- [[Azure Data Lake Gen2 - Access tiers]]
- [[Azure Data Lake Gen2 - Look at Access tiers]]
- [[Azure Data Lake Gen2 lifecycle policies]]
- [[View on Azure Monitor]]
-[[ Azure Data Factory - Alert Rules]]
- [[Azure Data Factory - Persisting pipeline runs]]
- [[Azure Data Factory - Annotations]]
- [[Azure Data Factory - Note on incremental data copy]]
- [[Azure Synapse Monitoring - Quick Overview]]
- [[Azure Synapse - System Views]]
- [[Azure Synapse - Workload Management]]
- [[Azure Synapse - Workload Management - Resources]]
- [[Azure Synapse transactions]]
- [[Azure Synapse - Transaction logging]]
- [[Azure Stream Analytics - Streaming Units]]
- [[Azure Stream Analytics - The importance of time]]
- [[Azure Stream Analytics - Metrics]]
- [[Azure Stream Analytics - Job Diagram]]
- [[Azure Event Hubs and Stream Analytics - Partitions]]
- [[Azure Databricks - Monitoring]]
- [[Azure Databricks - Sending logs to Azure Monitor]]
-[[ Azure Databricks - Pool]]